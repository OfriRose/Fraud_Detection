{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a2393331",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "38fbb92f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Data loaded successfully.\n"
     ]
    }
   ],
   "source": [
    "file_path = 'clensed_data.pkl'\n",
    "df = pd.read_pickle(file_path)\n",
    "\n",
    "print(\" Data loaded successfully.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ccbe586f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 2013945 entries, 0 to 17284449\n",
      "Data columns (total 23 columns):\n",
      " #   Column              Dtype         \n",
      "---  ------              -----         \n",
      " 0   Unnamed: 0          int64         \n",
      " 1   ssn                 string        \n",
      " 2   cc_num              int64         \n",
      " 3   city                string        \n",
      " 4   zip                 int64         \n",
      " 5   lat                 float64       \n",
      " 6   long                float64       \n",
      " 7   city_pop            float64       \n",
      " 8   job                 string        \n",
      " 9   dob                 datetime64[ns]\n",
      " 10  acct_num            int64         \n",
      " 11  profile             string        \n",
      " 12  trans_num           string        \n",
      " 13  trans_date          datetime64[ns]\n",
      " 14  category            string        \n",
      " 15  amt                 float64       \n",
      " 16  is_fraud            int64         \n",
      " 17  merchant            string        \n",
      " 18  merch_lat           float64       \n",
      " 19  merch_long          float64       \n",
      " 20  is_male             int64         \n",
      " 21  trans_timestamp     datetime64[ns]\n",
      " 22  IS_ANY_OUTLIER_IQR  int64         \n",
      "dtypes: datetime64[ns](3), float64(6), int64(7), string(7)\n",
      "memory usage: 368.8 MB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(df.info(verbose=True, max_cols=30))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18f4931e",
   "metadata": {},
   "source": [
    "##Date Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6e125129",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TX_MONTH feature created.\n",
      "TX_DAY feature created.\n",
      "TX_HOUR feature created.\n",
      "DOB_YEAR feature created.\n",
      "DOB_MONTH feature created.\n",
      "Age feature created.\n",
      "IS_WEEKEND feature created.\n"
     ]
    }
   ],
   "source": [
    "current_year = 2020\n",
    "\n",
    "df['TX_MONTH'] = df['trans_date'].dt.month\n",
    "print(\"TX_MONTH feature created.\")\n",
    "df['TX_DAY'] = df['trans_date'].dt.day\n",
    "print(\"TX_DAY feature created.\")\n",
    "df['TX_HOUR'] = df['trans_timestamp'].dt.hour\n",
    "print(\"TX_HOUR feature created.\")\n",
    "\n",
    "df['DOB_YEAR']= df['dob'].dt.year\n",
    "print(\"DOB_YEAR feature created.\")\n",
    "df['DOB_MONTH']= df['dob'].dt.month\n",
    "print(\"DOB_MONTH feature created.\")\n",
    "\n",
    "\n",
    "df['age'] = current_year - df['DOB_YEAR']\n",
    "print(\"Age feature created.\")\n",
    "df['IS_WEEKEND'] = np.where(df['trans_date'].dt.dayofweek >= 5, 1, 0)\n",
    "print(\"IS_WEEKEND feature created.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87066e21",
   "metadata": {},
   "source": [
    "##Time since last transaction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "727fb2c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'TIME_SINCE_LAST_TX' (in seconds) created.\n"
     ]
    }
   ],
   "source": [
    "df.sort_values(by=['cc_num', 'trans_timestamp'], inplace=True)\n",
    "\n",
    "df['TIME_SINCE_LAST_TX'] = (\n",
    "    df.groupby('cc_num')['trans_timestamp']\n",
    "      .diff()\n",
    "      .dt.total_seconds()\n",
    "      .fillna(0)\n",
    ")\n",
    "\n",
    "print(\"'TIME_SINCE_LAST_TX' (in seconds) created.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "555a2ca5",
   "metadata": {},
   "source": [
    "##Velocity Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a4f39b8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rolling Velocity Features (TX_COUNT_*, AMT_MAX_*, AMT_AVG_*) created.\n"
     ]
    }
   ],
   "source": [
    "df = df.reset_index(drop=True)\n",
    "df.sort_values(by=['cc_num', 'trans_timestamp'], inplace=True)\n",
    "\n",
    "WINDOWS = ['1h', '24h', '7d'] \n",
    "\n",
    "for window in WINDOWS:\n",
    "    tx_count_col = f'TX_COUNT_{window}'\n",
    "    amt_max_col = f'AMT_MAX_{window}'\n",
    "    amt_avg_col = f'AMT_AVG_{window}'\n",
    "\n",
    "    df[tx_count_col] = 0.0\n",
    "    df[amt_max_col] = 0.0\n",
    "    df[amt_avg_col] = 0.0\n",
    "\n",
    "    for cc_val, grp in df.groupby('cc_num', sort=False):\n",
    "        s = grp.set_index('trans_timestamp')['amt']\n",
    "\n",
    "        cnt = s.rolling(window=window).count().shift(1).fillna(0).values\n",
    "        mx = s.rolling(window=window).max().shift(1).fillna(0).values\n",
    "        avg = s.rolling(window=window).mean().shift(1).fillna(0).values\n",
    "\n",
    "        df.loc[grp.index, tx_count_col] = cnt\n",
    "        df.loc[grp.index, amt_max_col] = mx\n",
    "        df.loc[grp.index, amt_avg_col] = avg\n",
    "\n",
    "print(\"Rolling Velocity Features (TX_COUNT_*, AMT_MAX_*, AMT_AVG_*) created.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fae96819",
   "metadata": {},
   "source": [
    "##Relative amount"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c1059378",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Relative Amount Feature (AMT_vs_USER_AVG) created.\n"
     ]
    }
   ],
   "source": [
    "# overall lifetime average amount\n",
    "user_avg_amt = df.groupby('cc_num')['amt'].mean().rename('USER_LIFETIME_AVG_AMT')\n",
    "\n",
    "df = df.merge(user_avg_amt, on='cc_num', how='left')\n",
    "\n",
    "df['AMT_vs_USER_AVG'] = df['amt'] / df['USER_LIFETIME_AVG_AMT']\n",
    "df['AMT_vs_USER_AVG'] = df['AMT_vs_USER_AVG'].fillna(0)\n",
    "\n",
    "df.drop(columns=['USER_LIFETIME_AVG_AMT'], inplace=True)\n",
    "\n",
    "print(\"Relative Amount Feature (AMT_vs_USER_AVG) created.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83c88eb5",
   "metadata": {},
   "source": [
    "##Geospatial Features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "af030209",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Geospatial Distance Feature (DIST_HOME_MERCH_LOG) created.\n"
     ]
    }
   ],
   "source": [
    "from geopy.distance import great_circle\n",
    "\n",
    "def calculate_great_circle_distance(row):\n",
    "    \"\"\"Calculates distance in meters between customer home and merchant.\"\"\"\n",
    "    try:\n",
    "        customer_loc = (row['lat'], row['long'])\n",
    "        merchant_loc = (row['merch_lat'], row['merch_long'])\n",
    "        return great_circle(customer_loc, merchant_loc).meters\n",
    "    except Exception:\n",
    "        return 0 # Return 0 for bad coordinates\n",
    "\n",
    "df['DIST_HOME_MERCH'] = df.apply(calculate_great_circle_distance, axis=1)\n",
    "\n",
    "df['DIST_HOME_MERCH_LOG'] = np.log1p(df['DIST_HOME_MERCH'])\n",
    "df.drop(columns=['DIST_HOME_MERCH'], inplace=True)\n",
    "\n",
    "print(\"Geospatial Distance Feature (DIST_HOME_MERCH_LOG) created.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4b7b676c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'PROFILE_GEO_TYPE' created.\n"
     ]
    }
   ],
   "source": [
    "profile_components = df['profile'].str.split('_', expand=True)\n",
    "df['PROFILE_GEO_TYPE'] = profile_components[3]\n",
    "print(\"'PROFILE_GEO_TYPE' created.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0965fbe",
   "metadata": {},
   "source": [
    "##Transation Velocity Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "72d70bde",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating features for window: 1h\n",
      "Calculating features for window: 24h\n",
      "Calculating features for window: 7d\n",
      "Rolling Velocity Features created successfully.\n"
     ]
    }
   ],
   "source": [
    "df = df.reset_index(drop=True)\n",
    "\n",
    "df.sort_values(by=['cc_num', 'trans_timestamp'], inplace=True)\n",
    "\n",
    "WINDOWS = ['1h', '24h', '7d']\n",
    "\n",
    "for window in WINDOWS:\n",
    "    print(f\"Calculating features for window: {window}\")\n",
    "\n",
    "    tx_count_col = f'TX_COUNT_{window}'\n",
    "    amt_max_col = f'AMT_MAX_{window}'\n",
    "    amt_avg_col = f'AMT_AVG_{window}'\n",
    "\n",
    "    df[tx_count_col] = 0.0\n",
    "    df[amt_max_col] = 0.0\n",
    "    df[amt_avg_col] = 0.0\n",
    "\n",
    "    for cc_val, grp in df.groupby('cc_num', sort=False):\n",
    "        s = grp.set_index('trans_timestamp')['amt']\n",
    "\n",
    "        cnt = s.rolling(window=window).count().shift(1).fillna(0).values\n",
    "        mx = s.rolling(window=window).max().shift(1).fillna(0).values\n",
    "        avg = s.rolling(window=window).mean().shift(1).fillna(0).values\n",
    "\n",
    "        df.loc[grp.index, tx_count_col] = cnt\n",
    "        df.loc[grp.index, amt_max_col] = mx\n",
    "        df.loc[grp.index, amt_avg_col] = avg\n",
    "\n",
    "print(\"Rolling Velocity Features created successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f7f429fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'ZIP_CARD_COUNT_1D' (Unique Cards in 24h) created.\n"
     ]
    }
   ],
   "source": [
    "#Historical average of the is_fraud flag, grouped by zip code.\n",
    "\n",
    "df.sort_values(by=['trans_timestamp'], inplace=True)\n",
    "df = df.reset_index(drop=True)\n",
    "\n",
    "def zip_card_count_1d(group):\n",
    "    s = group.set_index('trans_timestamp')['cc_num']\n",
    "    res = s.rolling('24h').apply(lambda x: x.nunique(), raw=False).shift(1)\n",
    "    return pd.Series(res.values, index=group.index).fillna(0)\n",
    "\n",
    "df['ZIP_CARD_COUNT_1D'] = df.groupby('zip', group_keys=False).apply(zip_card_count_1d, include_groups=False).astype(float)\n",
    "\n",
    "print(\"'ZIP_CARD_COUNT_1D' (Unique Cards in 24h) created.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0888fcd",
   "metadata": {},
   "source": [
    "##SSN-Based Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "aa784f5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'SSN_SHARED_FLAG' created.\n"
     ]
    }
   ],
   "source": [
    "ssn_acct_counts = df.groupby('ssn')['acct_num'].nunique().rename('UNIQUE_ACCT_COUNT')\n",
    "df = df.merge(ssn_acct_counts, on='ssn', how='left')\n",
    "# 1 if the SSN is tied to more than one account, 0 otherwise\n",
    "df['SSN_SHARED_FLAG'] = np.where(df['UNIQUE_ACCT_COUNT'] > 1, 1, 0)\n",
    "\n",
    "df.drop(columns=['UNIQUE_ACCT_COUNT'], inplace=True)\n",
    "\n",
    "print(\"'SSN_SHARED_FLAG' created.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9e401b7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'SSN_COUNT_1D' (count in last 24 hours) created.\n"
     ]
    }
   ],
   "source": [
    "df = df.reset_index(drop=True)\n",
    "df['SSN_COUNT_1D'] = 0.0\n",
    "\n",
    "for ssn_val, grp in df.groupby('ssn', sort=False):\n",
    "    sorted_grp = grp.sort_values('trans_timestamp')\n",
    "    s = sorted_grp.set_index('trans_timestamp')['amt']\n",
    "    cnt = s.rolling(window='24h').count().shift(1).fillna(0).values\n",
    "    df.loc[sorted_grp.index, 'SSN_COUNT_1D'] = cnt\n",
    "\n",
    "print(\"'SSN_COUNT_1D' (count in last 24 hours) created.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "728f4d5a",
   "metadata": {},
   "source": [
    "##CC_NUM Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e87a57bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'CC_PREV_FRAUD' (Historical Fraud Count) created.\n"
     ]
    }
   ],
   "source": [
    "#finds if the cc number had previous frauds\n",
    "\n",
    "df.sort_values(by=['cc_num', 'trans_timestamp'], inplace=True)\n",
    "\n",
    "df['CC_CUM_FRAUD'] = df.groupby('cc_num')['is_fraud'].cumsum()\n",
    "df['CC_PREV_FRAUD'] = df['CC_CUM_FRAUD'].shift(1).fillna(0)\n",
    "\n",
    "df.drop(columns=['CC_CUM_FRAUD'], inplace=True)\n",
    "\n",
    "print(\"'CC_PREV_FRAUD' (Historical Fraud Count) created.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a02df9d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'CC_BIN' (Categorical BIN) created.\n"
     ]
    }
   ],
   "source": [
    "#card type\n",
    "df['CC_BIN'] = df['cc_num'][:6]\n",
    "\n",
    "print(\"'CC_BIN' (Categorical BIN) created.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d6c78c8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'CC_COUNT_LIFETIME' created.\n"
     ]
    }
   ],
   "source": [
    "# total count of transactions for each cc_num\n",
    "cc_lifetime_counts = df.groupby('cc_num')['amt'].count().rename('CC_COUNT_LIFETIME')\n",
    "df = df.merge(cc_lifetime_counts, on='cc_num', how='left')\n",
    "\n",
    "print(\"'CC_COUNT_LIFETIME' created.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b09beddc",
   "metadata": {},
   "source": [
    "##Binning\n",
    "#using quantile binning to deal to highly skewed data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6ce76f9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_and_drop_bins(df: pd.DataFrame, features_to_bin: list, num_bins: int = 5) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Applies quantile binning to a list of numerical features, creates new \n",
    "    categorical columns, and drops the original numerical columns.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): The input DataFrame.\n",
    "        features_to_bin (list): List of numerical column names to bin.\n",
    "        num_bins (int): The number of quantile bins to create.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: The DataFrame with new binned features and dropped originals.\n",
    "    \"\"\"\n",
    "    binned_features = []\n",
    "    dropped_originals = []\n",
    "    \n",
    "    print(f\"Starting Quantile Binning for {features_to_bin} into {num_bins} bins.\")\n",
    "\n",
    "    for col in features_to_bin:\n",
    "        new_col_name = f'{col}_BIN'\n",
    "        \n",
    "        if col not in df.columns:\n",
    "            print(f\"⚠️ Warning: Column '{col}' not found. Skipping.\")\n",
    "            continue\n",
    "            \n",
    "        try:\n",
    "            #Creates bins with equal number of records\n",
    "            df[new_col_name] = pd.qcut(\n",
    "                df[col], \n",
    "                q=num_bins, \n",
    "                labels=False, \n",
    "                duplicates='drop' \n",
    "            ).astype('category').astype(str) \n",
    "            \n",
    "            binned_features.append(new_col_name)\n",
    "            dropped_originals.append(col)\n",
    "            print(f\"  - ✅ '{col}' successfully binned into {df[new_col_name].nunique()} segments.\")\n",
    "            \n",
    "        except ValueError as e:\n",
    "            print(f\"  - ❌ Could not bin '{col}' into {num_bins} bins. Error: {e}\")\n",
    "            continue\n",
    "\n",
    "    df.drop(columns=dropped_originals, inplace=True, errors='ignore') \n",
    "    \n",
    "    print(\"\\n--- Binning Summary ---\")\n",
    "    print(f\" Binned Features Created: {binned_features}\")\n",
    "    print(f\" Original Numerical Features Dropped: {dropped_originals}\")\n",
    "    \n",
    "    return df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "754c78c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Quantile Binning for ['amt', 'city_pop', 'TIME_SINCE_LAST_TX', 'age', 'AMT_vs_USER_AVG', 'ZIP_CARD_COUNT_1D', 'AMT_AVG_7d', 'CC_COUNT_LIFETIME'] into 5 bins.\n",
      "  - ✅ 'amt' successfully binned into 5 segments.\n",
      "  - ✅ 'city_pop' successfully binned into 5 segments.\n",
      "  - ✅ 'TIME_SINCE_LAST_TX' successfully binned into 5 segments.\n",
      "  - ✅ 'age' successfully binned into 5 segments.\n",
      "  - ✅ 'AMT_vs_USER_AVG' successfully binned into 5 segments.\n",
      "  - ✅ 'ZIP_CARD_COUNT_1D' successfully binned into 5 segments.\n",
      "  - ✅ 'AMT_AVG_7d' successfully binned into 5 segments.\n",
      "  - ✅ 'CC_COUNT_LIFETIME' successfully binned into 5 segments.\n",
      "\n",
      "--- Binning Summary ---\n",
      " Binned Features Created: ['amt_BIN', 'city_pop_BIN', 'TIME_SINCE_LAST_TX_BIN', 'age_BIN', 'AMT_vs_USER_AVG_BIN', 'ZIP_CARD_COUNT_1D_BIN', 'AMT_AVG_7d_BIN', 'CC_COUNT_LIFETIME_BIN']\n",
      " Original Numerical Features Dropped: ['amt', 'city_pop', 'TIME_SINCE_LAST_TX', 'age', 'AMT_vs_USER_AVG', 'ZIP_CARD_COUNT_1D', 'AMT_AVG_7d', 'CC_COUNT_LIFETIME']\n"
     ]
    }
   ],
   "source": [
    "BINNING_CANDIDATES = [\n",
    "    'amt', \n",
    "    'city_pop', \n",
    "    'TIME_SINCE_LAST_TX',\n",
    "    'age',\n",
    "    'AMT_vs_USER_AVG', \n",
    "    'ZIP_CARD_COUNT_1D',\n",
    "    'AMT_AVG_7d', \n",
    "    'CC_COUNT_LIFETIME'\n",
    "]\n",
    "\n",
    "df = create_and_drop_bins(df, BINNING_CANDIDATES, num_bins=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab2f19cc",
   "metadata": {},
   "source": [
    "##Dropping original and useless features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1a0e7a17",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(columns=['profile','zip', 'long', 'lat', 'Unnamed: 0', 'cc_num', 'dob', 'acct_num', 'trans_num'\n",
    "                 ,'trans_date','merch_lat', 'merch_long', 'trans_timestamp','ssn','DOB_YEAR', 'DOB_MONTH'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e638abdf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>is_fraud</th>\n",
       "      <th>is_male</th>\n",
       "      <th>IS_ANY_OUTLIER_IQR</th>\n",
       "      <th>TX_MONTH</th>\n",
       "      <th>TX_DAY</th>\n",
       "      <th>TX_HOUR</th>\n",
       "      <th>IS_WEEKEND</th>\n",
       "      <th>TX_COUNT_1h</th>\n",
       "      <th>AMT_MAX_1h</th>\n",
       "      <th>AMT_AVG_1h</th>\n",
       "      <th>TX_COUNT_24h</th>\n",
       "      <th>AMT_MAX_24h</th>\n",
       "      <th>AMT_AVG_24h</th>\n",
       "      <th>TX_COUNT_7d</th>\n",
       "      <th>AMT_MAX_7d</th>\n",
       "      <th>DIST_HOME_MERCH_LOG</th>\n",
       "      <th>SSN_SHARED_FLAG</th>\n",
       "      <th>SSN_COUNT_1D</th>\n",
       "      <th>CC_PREV_FRAUD</th>\n",
       "      <th>CC_BIN</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>2.013945e+06</td>\n",
       "      <td>2.013945e+06</td>\n",
       "      <td>2.013945e+06</td>\n",
       "      <td>2.013945e+06</td>\n",
       "      <td>2.013945e+06</td>\n",
       "      <td>2.013945e+06</td>\n",
       "      <td>2.013945e+06</td>\n",
       "      <td>2.013945e+06</td>\n",
       "      <td>2.013945e+06</td>\n",
       "      <td>2.013945e+06</td>\n",
       "      <td>2.013945e+06</td>\n",
       "      <td>2.013945e+06</td>\n",
       "      <td>2.013945e+06</td>\n",
       "      <td>2.013945e+06</td>\n",
       "      <td>2.013945e+06</td>\n",
       "      <td>2.013945e+06</td>\n",
       "      <td>2013945.0</td>\n",
       "      <td>2.013945e+06</td>\n",
       "      <td>2.013945e+06</td>\n",
       "      <td>6.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>5.409284e-03</td>\n",
       "      <td>4.900392e-01</td>\n",
       "      <td>2.168748e-01</td>\n",
       "      <td>7.119759e+00</td>\n",
       "      <td>1.585131e+01</td>\n",
       "      <td>1.672458e+01</td>\n",
       "      <td>3.993048e-01</td>\n",
       "      <td>1.333607e+00</td>\n",
       "      <td>5.873726e+01</td>\n",
       "      <td>5.061403e+01</td>\n",
       "      <td>5.032191e+00</td>\n",
       "      <td>1.077685e+02</td>\n",
       "      <td>5.057659e+01</td>\n",
       "      <td>2.686795e+01</td>\n",
       "      <td>1.634760e+02</td>\n",
       "      <td>1.115439e+01</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.032191e+00</td>\n",
       "      <td>2.761014e+00</td>\n",
       "      <td>6.040566e+10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>7.334866e-02</td>\n",
       "      <td>4.999009e-01</td>\n",
       "      <td>4.121168e-01</td>\n",
       "      <td>3.423812e+00</td>\n",
       "      <td>8.837511e+00</td>\n",
       "      <td>4.570042e+00</td>\n",
       "      <td>4.897557e-01</td>\n",
       "      <td>6.252567e-01</td>\n",
       "      <td>5.434005e+01</td>\n",
       "      <td>4.786279e+01</td>\n",
       "      <td>3.253542e+00</td>\n",
       "      <td>5.661685e+01</td>\n",
       "      <td>2.990266e+01</td>\n",
       "      <td>1.488059e+01</td>\n",
       "      <td>3.022265e+01</td>\n",
       "      <td>5.166862e-01</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.253542e+00</td>\n",
       "      <td>4.686068e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>4.842894e+00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>6.040566e+10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>4.000000e+00</td>\n",
       "      <td>8.000000e+00</td>\n",
       "      <td>1.400000e+01</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>9.980000e+00</td>\n",
       "      <td>9.680000e+00</td>\n",
       "      <td>3.000000e+00</td>\n",
       "      <td>6.251000e+01</td>\n",
       "      <td>2.945000e+01</td>\n",
       "      <td>1.600000e+01</td>\n",
       "      <td>1.596500e+02</td>\n",
       "      <td>1.094452e+01</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>6.040566e+10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>7.000000e+00</td>\n",
       "      <td>1.600000e+01</td>\n",
       "      <td>1.700000e+01</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>4.320000e+01</td>\n",
       "      <td>3.673000e+01</td>\n",
       "      <td>4.000000e+00</td>\n",
       "      <td>1.057400e+02</td>\n",
       "      <td>4.722000e+01</td>\n",
       "      <td>2.400000e+01</td>\n",
       "      <td>1.793950e+02</td>\n",
       "      <td>1.129042e+01</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>6.040566e+10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>1.000000e+01</td>\n",
       "      <td>2.400000e+01</td>\n",
       "      <td>2.000000e+01</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>2.000000e+00</td>\n",
       "      <td>8.926000e+01</td>\n",
       "      <td>7.570000e+01</td>\n",
       "      <td>7.000000e+00</td>\n",
       "      <td>1.726700e+02</td>\n",
       "      <td>6.683500e+01</td>\n",
       "      <td>3.500000e+01</td>\n",
       "      <td>1.793950e+02</td>\n",
       "      <td>1.151035e+01</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.000000e+00</td>\n",
       "      <td>7.000000e+00</td>\n",
       "      <td>6.040566e+10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.200000e+01</td>\n",
       "      <td>3.100000e+01</td>\n",
       "      <td>2.300000e+01</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>9.000000e+00</td>\n",
       "      <td>1.793950e+02</td>\n",
       "      <td>1.793950e+02</td>\n",
       "      <td>3.900000e+01</td>\n",
       "      <td>1.793950e+02</td>\n",
       "      <td>1.793950e+02</td>\n",
       "      <td>1.090000e+02</td>\n",
       "      <td>1.793950e+02</td>\n",
       "      <td>1.188653e+01</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.900000e+01</td>\n",
       "      <td>1.600000e+01</td>\n",
       "      <td>6.040566e+10</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           is_fraud       is_male  IS_ANY_OUTLIER_IQR      TX_MONTH  \\\n",
       "count  2.013945e+06  2.013945e+06        2.013945e+06  2.013945e+06   \n",
       "mean   5.409284e-03  4.900392e-01        2.168748e-01  7.119759e+00   \n",
       "std    7.334866e-02  4.999009e-01        4.121168e-01  3.423812e+00   \n",
       "min    0.000000e+00  0.000000e+00        0.000000e+00  1.000000e+00   \n",
       "25%    0.000000e+00  0.000000e+00        0.000000e+00  4.000000e+00   \n",
       "50%    0.000000e+00  0.000000e+00        0.000000e+00  7.000000e+00   \n",
       "75%    0.000000e+00  1.000000e+00        0.000000e+00  1.000000e+01   \n",
       "max    1.000000e+00  1.000000e+00        1.000000e+00  1.200000e+01   \n",
       "\n",
       "             TX_DAY       TX_HOUR    IS_WEEKEND   TX_COUNT_1h    AMT_MAX_1h  \\\n",
       "count  2.013945e+06  2.013945e+06  2.013945e+06  2.013945e+06  2.013945e+06   \n",
       "mean   1.585131e+01  1.672458e+01  3.993048e-01  1.333607e+00  5.873726e+01   \n",
       "std    8.837511e+00  4.570042e+00  4.897557e-01  6.252567e-01  5.434005e+01   \n",
       "min    1.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00   \n",
       "25%    8.000000e+00  1.400000e+01  0.000000e+00  1.000000e+00  9.980000e+00   \n",
       "50%    1.600000e+01  1.700000e+01  0.000000e+00  1.000000e+00  4.320000e+01   \n",
       "75%    2.400000e+01  2.000000e+01  1.000000e+00  2.000000e+00  8.926000e+01   \n",
       "max    3.100000e+01  2.300000e+01  1.000000e+00  9.000000e+00  1.793950e+02   \n",
       "\n",
       "         AMT_AVG_1h  TX_COUNT_24h   AMT_MAX_24h   AMT_AVG_24h   TX_COUNT_7d  \\\n",
       "count  2.013945e+06  2.013945e+06  2.013945e+06  2.013945e+06  2.013945e+06   \n",
       "mean   5.061403e+01  5.032191e+00  1.077685e+02  5.057659e+01  2.686795e+01   \n",
       "std    4.786279e+01  3.253542e+00  5.661685e+01  2.990266e+01  1.488059e+01   \n",
       "min    0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00   \n",
       "25%    9.680000e+00  3.000000e+00  6.251000e+01  2.945000e+01  1.600000e+01   \n",
       "50%    3.673000e+01  4.000000e+00  1.057400e+02  4.722000e+01  2.400000e+01   \n",
       "75%    7.570000e+01  7.000000e+00  1.726700e+02  6.683500e+01  3.500000e+01   \n",
       "max    1.793950e+02  3.900000e+01  1.793950e+02  1.793950e+02  1.090000e+02   \n",
       "\n",
       "         AMT_MAX_7d  DIST_HOME_MERCH_LOG  SSN_SHARED_FLAG  SSN_COUNT_1D  \\\n",
       "count  2.013945e+06         2.013945e+06        2013945.0  2.013945e+06   \n",
       "mean   1.634760e+02         1.115439e+01              0.0  5.032191e+00   \n",
       "std    3.022265e+01         5.166862e-01              0.0  3.253542e+00   \n",
       "min    0.000000e+00         4.842894e+00              0.0  0.000000e+00   \n",
       "25%    1.596500e+02         1.094452e+01              0.0  3.000000e+00   \n",
       "50%    1.793950e+02         1.129042e+01              0.0  4.000000e+00   \n",
       "75%    1.793950e+02         1.151035e+01              0.0  7.000000e+00   \n",
       "max    1.793950e+02         1.188653e+01              0.0  3.900000e+01   \n",
       "\n",
       "       CC_PREV_FRAUD        CC_BIN  \n",
       "count   2.013945e+06  6.000000e+00  \n",
       "mean    2.761014e+00  6.040566e+10  \n",
       "std     4.686068e+00  0.000000e+00  \n",
       "min     0.000000e+00  6.040566e+10  \n",
       "25%     0.000000e+00  6.040566e+10  \n",
       "50%     0.000000e+00  6.040566e+10  \n",
       "75%     7.000000e+00  6.040566e+10  \n",
       "max     1.600000e+01  6.040566e+10  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42eaee5c",
   "metadata": {},
   "source": [
    "##Hot One Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4b267d29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoding Complete. Final feature count: 173\n"
     ]
    }
   ],
   "source": [
    "CATEGORICAL_FEATURES = ['category','city', 'TX_MONTH', 'TX_HOUR','TX_DAY', 'amt_BIN', 'city_pop_BIN', 'TIME_SINCE_LAST_TX_BIN', 'age_BIN',\n",
    "                         'merchant', 'job', 'PROFILE_GEO_TYPE', 'CC_BIN' ]\n",
    "\n",
    "df = pd.get_dummies(\n",
    "    df,\n",
    "    columns=CATEGORICAL_FEATURES,\n",
    "    prefix=CATEGORICAL_FEATURES,\n",
    "    drop_first=True \n",
    ")\n",
    "print(f\"Encoding Complete. Final feature count: {df.shape[1] - 1}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96e5411b",
   "metadata": {},
   "source": [
    "##Standardization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d593bc9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting final Standardization for all features...\n",
      "Final Standardization Complete.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "X = df.drop(columns=['is_fraud'])\n",
    "y = df['is_fraud']\n",
    "\n",
    "ALL_FEATURES = X.columns.tolist()\n",
    "\n",
    "print(\"Starting final Standardization for all features...\")\n",
    "scaler = StandardScaler()\n",
    "\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "X_scaled = pd.DataFrame(X_scaled, index=X.index, columns=ALL_FEATURES)\n",
    "df = pd.concat([X_scaled, y], axis=1)\n",
    "print(\"Final Standardization Complete.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8b93428",
   "metadata": {},
   "source": [
    "##Unbalanced Data fitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8cf414d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Samples: 2,013,945\n",
      "Training set size: 1,409,761 (70.00%)\n",
      "Validation (Dev) set size: 302,092 (15.00%)\n",
      "Testing set size: 302,092 (15.00%)\n",
      "\n",
      "Data split successfully with stratification.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "X = df.drop(columns=['is_fraud'])\n",
    "y = df['is_fraud'] \n",
    "\n",
    "total_samples = len(df)\n",
    "\n",
    "\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(\n",
    "    X, y, \n",
    "    test_size=0.3, \n",
    "    random_state=42, \n",
    "    stratify=y \n",
    ")\n",
    "\n",
    "\n",
    "X_dev, X_test, y_dev, y_test = train_test_split(\n",
    "    X_temp, y_temp, \n",
    "    test_size=0.5,\n",
    "    random_state=42, \n",
    "    stratify=y_temp \n",
    ")\n",
    "\n",
    "train_percent = (X_train.shape[0] / total_samples) * 100\n",
    "dev_percent = (X_dev.shape[0] / total_samples) * 100\n",
    "test_percent = (X_test.shape[0] / total_samples) * 100\n",
    "\n",
    "print(f\"Total Samples: {total_samples:,}\")\n",
    "print(f\"Training set size: {X_train.shape[0]:,} ({train_percent:.2f}%)\")\n",
    "print(f\"Validation (Dev) set size: {X_dev.shape[0]:,} ({dev_percent:.2f}%)\")\n",
    "print(f\"Testing set size: {X_test.shape[0]:,} ({test_percent:.2f}%)\")\n",
    "print(\"\\nData split successfully with stratification.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3be14b1c",
   "metadata": {},
   "source": [
    "##Feature Selection:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "371d89d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Consensus Feature Selection...\n",
      "-> Fitting Lasso_L1...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda/lib/python3.13/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-> Fitting Ridge_L2...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda/lib/python3.13/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-> Fitting GradientBoost...\n",
      "-> Fitting RandomForest...\n",
      "Filtering down to the absolute top 20 features...\n",
      "✅ Final feature count selected: 20\n",
      "\n",
      "Top 20 Features Selected:\n",
      "| Feature                |   Sum |\n",
      "|:-----------------------|------:|\n",
      "| IS_ANY_OUTLIER_IQR     |     4 |\n",
      "| AMT_MAX_1h             |     4 |\n",
      "| TX_COUNT_1h            |     4 |\n",
      "| TX_COUNT_24h           |     4 |\n",
      "| AMT_AVG_1h             |     4 |\n",
      "| TX_COUNT_7d            |     4 |\n",
      "| DIST_HOME_MERCH_LOG    |     4 |\n",
      "| AMT_MAX_24h            |     4 |\n",
      "| AMT_AVG_24h            |     4 |\n",
      "| AMT_AVG_7d_BIN         |     4 |\n",
      "| category_food_dining   |     4 |\n",
      "| CC_PREV_FRAUD          |     4 |\n",
      "| SSN_COUNT_1D           |     4 |\n",
      "| category_misc_net      |     4 |\n",
      "| category_travel        |     4 |\n",
      "| category_shopping_pos  |     4 |\n",
      "| category_shopping_net  |     4 |\n",
      "| category_gas_transport |     4 |\n",
      "| category_misc_pos      |     4 |\n",
      "| category_grocery_pos   |     4 |\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.ensemble import GradientBoostingClassifier, RandomForestClassifier\n",
    "\n",
    "\n",
    "imbalance_ratio = (y_train == 0).sum() / (y_train == 1).sum()\n",
    "class_weights = {0: 1, 1: imbalance_ratio}\n",
    "\n",
    "models = {\n",
    "    # 1. Lasso (L1 penalty)\n",
    "    'Lasso_L1': LogisticRegression(penalty='l1', solver='saga', C=0.01, class_weight=class_weights, random_state=42, max_iter=5000),\n",
    "    \n",
    "    # 2. Ridge (L2 penalty)\n",
    "    'Ridge_L2': LogisticRegression(penalty='l2', solver='saga', C=0.01, class_weight=class_weights, random_state=42, max_iter=5000),\n",
    "    \n",
    "    # 3. Linear SVM (L1 penalty) - rmoved due to long training times\n",
    "    #'SVM_L1': LinearSVC(C=0.01, penalty=\"l1\", dual=False, class_weight=class_weights, random_state=42, max_iter=2000),\n",
    "    \n",
    "    # 4. Gradient Boosting\n",
    "    'GradientBoost': GradientBoostingClassifier(n_estimators=50, max_depth=4, random_state=42),\n",
    "    \n",
    "    # 5. Random Forest\n",
    "    'RandomForest': RandomForestClassifier(n_estimators=50, max_depth=8, class_weight=class_weights, random_state=42, n_jobs=-1),\n",
    "}\n",
    "\n",
    "selection_results = {}\n",
    "print(\"Starting Consensus Feature Selection...\")\n",
    "\n",
    "for name, model in models.items():\n",
    "    print(f\"-> Fitting {name}...\")\n",
    "    \n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    if hasattr(model, 'coef_'):\n",
    "        coef = model.coef_[0] if model.coef_.ndim > 1 else model.coef_\n",
    "        selected = (np.abs(coef) > 1e-4).astype(int)\n",
    "    elif hasattr(model, 'feature_importances_'):\n",
    "        selected = (model.feature_importances_ > 1e-4).astype(int)\n",
    "    else:\n",
    "        selected = np.zeros(X_train.shape[1]).astype(int)\n",
    "    \n",
    "    selection_results[name] = selected\n",
    "\n",
    "selection_df = pd.DataFrame(selection_results, index=X_train.columns)\n",
    "selection_df.index.name = 'Feature'\n",
    "\n",
    "selection_df['Sum'] = selection_df.sum(axis=1)\n",
    "selection_df = selection_df.sort_values(by='Sum', ascending=False)\n",
    "\n",
    "TOP_N_FEATURES = 20\n",
    "print(f\"Filtering down to the absolute top {TOP_N_FEATURES} features...\")\n",
    "\n",
    "final_selected_features_df = selection_df.sort_values(\n",
    "    by='Sum', \n",
    "    ascending=False\n",
    ").head(TOP_N_FEATURES)\n",
    "\n",
    "final_selected_features = final_selected_features_df.index.tolist()\n",
    "\n",
    "print(f\"✅ Final feature count selected: {len(final_selected_features)}\")\n",
    "print(\"\\nTop 20 Features Selected:\")\n",
    "print(final_selected_features_df[['Sum']].to_markdown())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "7b0fb23a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Final, optimized, and filtered data sets saved for Modeling.\n"
     ]
    }
   ],
   "source": [
    "SELECTED_FEATURES = final_selected_features\n",
    "\n",
    "X_train_filtered = X_train[SELECTED_FEATURES]\n",
    "X_dev_filtered = X_dev[SELECTED_FEATURES]\n",
    "X_test_filtered = X_test[SELECTED_FEATURES]\n",
    "\n",
    "np.save('selected_features.npy', np.array(SELECTED_FEATURES))\n",
    "\n",
    "df_train_final = pd.concat([X_train_filtered, y_train], axis=1)\n",
    "df_dev_final = pd.concat([X_dev_filtered, y_dev], axis=1)\n",
    "df_test_final = pd.concat([X_test_filtered, y_test], axis=1)\n",
    "\n",
    "df_train_final.to_pickle('train_set_final_filtered.pkl')\n",
    "df_dev_final.to_pickle('dev_set_final_filtered.pkl')\n",
    "df_test_final.to_pickle('test_set_final_filtered.pkl')\n",
    "\n",
    "print(\"\\n Final, optimized, and filtered data sets saved for Modeling.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
