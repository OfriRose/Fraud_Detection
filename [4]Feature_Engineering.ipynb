{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "08882b98",
   "metadata": {},
   "source": [
    "#age\n",
    "#city population bins\n",
    "#time from last buy\n",
    "#num of trans last transaction\n",
    "#average amt per user\n",
    "#check if the accounts commited fraud before\n",
    "#SSN_State_Mismatch_Flag\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a2393331",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "38fbb92f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Data loaded successfully.\n"
     ]
    }
   ],
   "source": [
    "file_path = 'clensed_data.pkl'\n",
    "df = pd.read_pickle(file_path)\n",
    "\n",
    "print(\" Data loaded successfully.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ccbe586f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 2013945 entries, 0 to 17284449\n",
      "Data columns (total 23 columns):\n",
      " #   Column              Dtype         \n",
      "---  ------              -----         \n",
      " 0   Unnamed: 0          int64         \n",
      " 1   ssn                 string        \n",
      " 2   cc_num              int64         \n",
      " 3   city                string        \n",
      " 4   zip                 int64         \n",
      " 5   lat                 float64       \n",
      " 6   long                float64       \n",
      " 7   city_pop            float64       \n",
      " 8   job                 string        \n",
      " 9   dob                 datetime64[ns]\n",
      " 10  acct_num            int64         \n",
      " 11  profile             string        \n",
      " 12  trans_num           string        \n",
      " 13  trans_date          datetime64[ns]\n",
      " 14  category            string        \n",
      " 15  amt                 float64       \n",
      " 16  is_fraud            int64         \n",
      " 17  merchant            string        \n",
      " 18  merch_lat           float64       \n",
      " 19  merch_long          float64       \n",
      " 20  is_male             int64         \n",
      " 21  trans_timestamp     datetime64[ns]\n",
      " 22  IS_ANY_OUTLIER_IQR  int64         \n",
      "dtypes: datetime64[ns](3), float64(6), int64(7), string(7)\n",
      "memory usage: 368.8 MB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(df.info(verbose=True, max_cols=30))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18f4931e",
   "metadata": {},
   "source": [
    "Date Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6e125129",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TX_MONTH feature created.\n",
      "TX_DAY feature created.\n",
      "TX_HOUR feature created.\n",
      "DOB_YEAR feature created.\n",
      "DOB_MONTH feature created.\n",
      "Age feature created.\n",
      "IS_WEEKEND feature created.\n"
     ]
    }
   ],
   "source": [
    "current_year = 2020\n",
    "\n",
    "df['TX_MONTH'] = df['trans_date'].dt.month\n",
    "print(\"TX_MONTH feature created.\")\n",
    "df['TX_DAY'] = df['trans_date'].dt.day\n",
    "print(\"TX_DAY feature created.\")\n",
    "df['TX_HOUR'] = df['trans_timestamp'].dt.hour\n",
    "print(\"TX_HOUR feature created.\")\n",
    "\n",
    "df['DOB_YEAR']= df['dob'].dt.year\n",
    "print(\"DOB_YEAR feature created.\")\n",
    "df['DOB_MONTH']= df['dob'].dt.month\n",
    "print(\"DOB_MONTH feature created.\")\n",
    "\n",
    "\n",
    "df['age'] = current_year - df['DOB_YEAR']\n",
    "print(\"Age feature created.\")\n",
    "df['IS_WEEKEND'] = np.where(df['trans_date'].dt.dayofweek >= 5, 1, 0)\n",
    "print(\"IS_WEEKEND feature created.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87066e21",
   "metadata": {},
   "source": [
    "Time since last transaction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "727fb2c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'TIME_SINCE_LAST_TX' (in seconds) created.\n"
     ]
    }
   ],
   "source": [
    "df.sort_values(by=['cc_num', 'trans_timestamp'], inplace=True)\n",
    "\n",
    "df['TIME_SINCE_LAST_TX'] = (\n",
    "    df.groupby('cc_num')['trans_timestamp']\n",
    "      .diff()\n",
    "      .dt.total_seconds()\n",
    "      .fillna(0)\n",
    ")\n",
    "\n",
    "print(\"'TIME_SINCE_LAST_TX' (in seconds) created.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "555a2ca5",
   "metadata": {},
   "source": [
    "Velocity Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a4f39b8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rolling Velocity Features (TX_COUNT_*, AMT_MAX_*, AMT_AVG_*) created.\n"
     ]
    }
   ],
   "source": [
    "df = df.reset_index(drop=True)\n",
    "df.sort_values(by=['cc_num', 'trans_timestamp'], inplace=True)\n",
    "\n",
    "WINDOWS = ['1h', '24h', '7d'] \n",
    "\n",
    "for window in WINDOWS:\n",
    "    tx_count_col = f'TX_COUNT_{window}'\n",
    "    amt_max_col = f'AMT_MAX_{window}'\n",
    "    amt_avg_col = f'AMT_AVG_{window}'\n",
    "\n",
    "    df[tx_count_col] = 0.0\n",
    "    df[amt_max_col] = 0.0\n",
    "    df[amt_avg_col] = 0.0\n",
    "\n",
    "    for cc_val, grp in df.groupby('cc_num', sort=False):\n",
    "        s = grp.set_index('trans_timestamp')['amt']\n",
    "\n",
    "        cnt = s.rolling(window=window).count().shift(1).fillna(0).values\n",
    "        mx = s.rolling(window=window).max().shift(1).fillna(0).values\n",
    "        avg = s.rolling(window=window).mean().shift(1).fillna(0).values\n",
    "\n",
    "        df.loc[grp.index, tx_count_col] = cnt\n",
    "        df.loc[grp.index, amt_max_col] = mx\n",
    "        df.loc[grp.index, amt_avg_col] = avg\n",
    "\n",
    "print(\"Rolling Velocity Features (TX_COUNT_*, AMT_MAX_*, AMT_AVG_*) created.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fae96819",
   "metadata": {},
   "source": [
    "Relative amount"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c1059378",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_131040/2710787939.py:7: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df['AMT_vs_USER_AVG'].fillna(0, inplace=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Relative Amount Feature (AMT_vs_USER_AVG) created.\n"
     ]
    }
   ],
   "source": [
    "# overall lifetime average amount\n",
    "user_avg_amt = df.groupby('cc_num')['amt'].mean().rename('USER_LIFETIME_AVG_AMT')\n",
    "\n",
    "df = df.merge(user_avg_amt, on='cc_num', how='left')\n",
    "\n",
    "df['AMT_vs_USER_AVG'] = df['amt'] / df['USER_LIFETIME_AVG_AMT']\n",
    "df['AMT_vs_USER_AVG'].fillna(0, inplace=True) \n",
    "\n",
    "df.drop(columns=['USER_LIFETIME_AVG_AMT'], inplace=True)\n",
    "\n",
    "print(\"Relative Amount Feature (AMT_vs_USER_AVG) created.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83c88eb5",
   "metadata": {},
   "source": [
    "Geospatial Features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "af030209",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Geospatial Distance Feature (DIST_HOME_MERCH_LOG) created.\n"
     ]
    }
   ],
   "source": [
    "from geopy.distance import great_circle\n",
    "\n",
    "def calculate_great_circle_distance(row):\n",
    "    \"\"\"Calculates distance in meters between customer home and merchant.\"\"\"\n",
    "    try:\n",
    "        customer_loc = (row['lat'], row['long'])\n",
    "        merchant_loc = (row['merch_lat'], row['merch_long'])\n",
    "        return great_circle(customer_loc, merchant_loc).meters\n",
    "    except Exception:\n",
    "        return 0 # Return 0 for bad coordinates\n",
    "\n",
    "df['DIST_HOME_MERCH'] = df.apply(calculate_great_circle_distance, axis=1)\n",
    "\n",
    "df['DIST_HOME_MERCH_LOG'] = np.log1p(df['DIST_HOME_MERCH'])\n",
    "df.drop(columns=['DIST_HOME_MERCH'], inplace=True)\n",
    "\n",
    "print(\"Geospatial Distance Feature (DIST_HOME_MERCH_LOG) created.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4b7b676c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'PROFILE_GEO_TYPE' created.\n"
     ]
    }
   ],
   "source": [
    "profile_components = df['profile'].str.split('_', expand=True)\n",
    "df['PROFILE_GEO_TYPE'] = profile_components[3]\n",
    "print(\"'PROFILE_GEO_TYPE' created.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0965fbe",
   "metadata": {},
   "source": [
    "Transation Velocity Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72d70bde",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating features for window: 1h\n"
     ]
    }
   ],
   "source": [
    "df = df.reset_index(drop=True)\n",
    "\n",
    "df.sort_values(by=['cc_num', 'trans_timestamp'], inplace=True)\n",
    "\n",
    "WINDOWS = ['1h', '24h', '7d']\n",
    "\n",
    "for window in WINDOWS:\n",
    "    print(f\"Calculating features for window: {window}\")\n",
    "\n",
    "    tx_count_col = f'TX_COUNT_{window}'\n",
    "    amt_max_col = f'AMT_MAX_{window}'\n",
    "    amt_avg_col = f'AMT_AVG_{window}'\n",
    "\n",
    "    df[tx_count_col] = 0.0\n",
    "    df[amt_max_col] = 0.0\n",
    "    df[amt_avg_col] = 0.0\n",
    "\n",
    "    for cc_val, grp in df.groupby('cc_num', sort=False):\n",
    "        s = grp.set_index('trans_timestamp')['amt']\n",
    "\n",
    "        cnt = s.rolling(window=window).count().shift(1).fillna(0).values\n",
    "        mx = s.rolling(window=window).max().shift(1).fillna(0).values\n",
    "        avg = s.rolling(window=window).mean().shift(1).fillna(0).values\n",
    "\n",
    "        df.loc[grp.index, tx_count_col] = cnt\n",
    "        df.loc[grp.index, amt_max_col] = mx\n",
    "        df.loc[grp.index, amt_avg_col] = avg\n",
    "\n",
    "print(\"Rolling Velocity Features created successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7f429fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Historical average of the is_fraud flag, grouped by zip code.\n",
    "\n",
    "df.sort_values(by=['trans_timestamp'], inplace=True)\n",
    "df = df.reset_index(drop=True)\n",
    "\n",
    "def zip_card_count_1d(group):\n",
    "    s = group.set_index('trans_timestamp')['cc_num']\n",
    "    res = s.rolling('24h').apply(lambda x: x.nunique(), raw=False).shift(1)\n",
    "    return pd.Series(res.values, index=group.index).fillna(0)\n",
    "\n",
    "df['ZIP_CARD_COUNT_1D'] = df.groupby('zip', group_keys=False).apply(zip_card_count_1d).astype(float)\n",
    "\n",
    "print(\"'ZIP_CARD_COUNT_1D' (Unique Cards in 24h) created.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0888fcd",
   "metadata": {},
   "source": [
    "SSN-Based Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa784f5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "ssn_acct_counts = df.groupby('ssn')['acct_num'].nunique().rename('UNIQUE_ACCT_COUNT')\n",
    "df = df.merge(ssn_acct_counts, on='ssn', how='left')\n",
    "# 1 if the SSN is tied to more than one account, 0 otherwise\n",
    "df['SSN_SHARED_FLAG'] = np.where(df['UNIQUE_ACCT_COUNT'] > 1, 1, 0)\n",
    "\n",
    "df.drop(columns=['UNIQUE_ACCT_COUNT'], inplace=True)\n",
    "\n",
    "print(\"'SSN_SHARED_FLAG' created.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e401b7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.reset_index(drop=True)\n",
    "df['SSN_COUNT_1D'] = 0.0\n",
    "\n",
    "for ssn_val, grp in df.groupby('ssn', sort=False):\n",
    "    sorted_grp = grp.sort_values('trans_timestamp')\n",
    "    s = sorted_grp.set_index('trans_timestamp')['amt']\n",
    "    cnt = s.rolling(window='24h').count().shift(1).fillna(0).values\n",
    "    df.loc[sorted_grp.index, 'SSN_COUNT_1D'] = cnt\n",
    "\n",
    "print(\"'SSN_COUNT_1D' (count in last 24 hours) created.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "728f4d5a",
   "metadata": {},
   "source": [
    "CC_NUM Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e87a57bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#finds if the cc number had previous frauds\n",
    "\n",
    "df.sort_values(by=['cc_num', 'trans_timestamp'], inplace=True)\n",
    "\n",
    "df['CC_CUM_FRAUD'] = df.groupby('cc_num')['is_fraud'].cumsum()\n",
    "df['CC_PREV_FRAUD'] = df['CC_CUM_FRAUD'].shift(1).fillna(0)\n",
    "\n",
    "df.drop(columns=['CC_CUM_FRAUD'], inplace=True)\n",
    "\n",
    "print(\"'CC_PREV_FRAUD' (Historical Fraud Count) created.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a02df9d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#card type\n",
    "df['CC_BIN'] = df['cc_num'][:6]\n",
    "\n",
    "print(\"'CC_BIN' (Categorical BIN) created.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6c78c8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# total count of transactions for each cc_num\n",
    "cc_lifetime_counts = df.groupby('cc_num')['amt'].count().rename('CC_COUNT_LIFETIME')\n",
    "df = df.merge(cc_lifetime_counts, on='cc_num', how='left')\n",
    "\n",
    "print(\"'CC_COUNT_LIFETIME' created.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b09beddc",
   "metadata": {},
   "source": [
    "Binning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ce76f9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_and_drop_bins(df: pd.DataFrame, features_to_bin: list, num_bins: int = 5) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Applies quantile binning to a list of numerical features, creates new \n",
    "    categorical columns, and drops the original numerical columns.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): The input DataFrame.\n",
    "        features_to_bin (list): List of numerical column names to bin.\n",
    "        num_bins (int): The number of quantile bins to create.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: The DataFrame with new binned features and dropped originals.\n",
    "    \"\"\"\n",
    "    binned_features = []\n",
    "    dropped_originals = []\n",
    "    \n",
    "    print(f\"Starting Quantile Binning for {features_to_bin} into {num_bins} bins.\")\n",
    "\n",
    "    for col in features_to_bin:\n",
    "        new_col_name = f'{col}_BIN'\n",
    "        \n",
    "        if col not in df.columns:\n",
    "            print(f\"⚠️ Warning: Column '{col}' not found. Skipping.\")\n",
    "            continue\n",
    "            \n",
    "        try:\n",
    "            #Creates bins with equal number of records\n",
    "            df[new_col_name] = pd.qcut(\n",
    "                df[col], \n",
    "                q=num_bins, \n",
    "                labels=False, \n",
    "                duplicates='drop' \n",
    "            ).astype('category').astype(str) \n",
    "            \n",
    "            binned_features.append(new_col_name)\n",
    "            dropped_originals.append(col)\n",
    "            print(f\"  - ✅ '{col}' successfully binned into {df[new_col_name].nunique()} segments.\")\n",
    "            \n",
    "        except ValueError as e:\n",
    "            print(f\"  - ❌ Could not bin '{col}' into {num_bins} bins. Error: {e}\")\n",
    "            continue\n",
    "\n",
    "    df.drop(columns=dropped_originals, inplace=True, errors='ignore') \n",
    "    \n",
    "    print(\"\\n--- Binning Summary ---\")\n",
    "    print(f\"✅ Binned Features Created: {binned_features}\")\n",
    "    print(f\"✅ Original Numerical Features Dropped: {dropped_originals}\")\n",
    "    \n",
    "    return df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "754c78c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "BINNING_CANDIDATES = [\n",
    "    'amt', \n",
    "    'city_pop', \n",
    "    'TIME_SINCE_LAST_TX',\n",
    "    'age',\n",
    "    'AMT_vs_USER_AVG', \n",
    "    'ZIP_CARD_COUNT_1D',\n",
    "    'AMT_AVG_7d', \n",
    "    'CC_COUNT_LIFETIME'\n",
    "]\n",
    "\n",
    "df = create_and_drop_bins(df, BINNING_CANDIDATES, num_bins=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab2f19cc",
   "metadata": {},
   "source": [
    "Dropping original features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a0e7a17",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(columns=['profile','zip', 'long', 'lat', 'Unnamed: 0', 'cc_num', 'dob', 'acct_num', 'trans_num'\n",
    "                 ,'trans_date','merch_lat', 'merch_long', 'trans_timestamp','ssn','DOB_YEAR', 'DOB_MONTH'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e638abdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42eaee5c",
   "metadata": {},
   "source": [
    "##Hot One Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b267d29",
   "metadata": {},
   "outputs": [],
   "source": [
    "CATEGORICAL_FEATURES = ['category','city', 'TX_MONTH', 'TX_HOUR','TX_DAY', 'amt_BIN', 'city_pop_BIN', 'TIME_SINCE_LAST_TX_BIN', 'age_BIN',\n",
    "                         'merchant', 'job', 'PROFILE_GEO_TYPE', 'CC_BIN' ]\n",
    "\n",
    "df = pd.get_dummies(\n",
    "    df,\n",
    "    columns=CATEGORICAL_FEATURES,\n",
    "    prefix=CATEGORICAL_FEATURES,\n",
    "    drop_first=True \n",
    ")\n",
    "print(f\"Encoding Complete. Final feature count: {df.shape[1] - 1}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96e5411b",
   "metadata": {},
   "source": [
    "Standardization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d593bc9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "X = df.drop(columns=['is_fraud'])\n",
    "y = df['is_fraud']\n",
    "\n",
    "ALL_FEATURES = X.columns.tolist()\n",
    "\n",
    "print(\"Starting final Standardization for all features...\")\n",
    "scaler = StandardScaler()\n",
    "\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "X_scaled = pd.DataFrame(X_scaled, index=X.index, columns=ALL_FEATURES)\n",
    "df = pd.concat([X_scaled, y], axis=1)\n",
    "print(\"Final Standardization Complete.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8b93428",
   "metadata": {},
   "source": [
    "Unbalanced Data fitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cf414d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "X = df.drop(columns=['is_fraud'])\n",
    "y = df['is_fraud'] \n",
    "\n",
    "total_samples = len(df)\n",
    "\n",
    "\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(\n",
    "    X, y, \n",
    "    test_size=0.3, \n",
    "    random_state=42, \n",
    "    stratify=y \n",
    ")\n",
    "\n",
    "\n",
    "X_dev, X_test, y_dev, y_test = train_test_split(\n",
    "    X_temp, y_temp, \n",
    "    test_size=0.5,\n",
    "    random_state=42, \n",
    "    stratify=y_temp \n",
    ")\n",
    "\n",
    "train_percent = (X_train.shape[0] / total_samples) * 100\n",
    "dev_percent = (X_dev.shape[0] / total_samples) * 100\n",
    "test_percent = (X_test.shape[0] / total_samples) * 100\n",
    "\n",
    "print(f\"Total Samples: {total_samples:,}\")\n",
    "print(f\"Training set size: {X_train.shape[0]:,} ({train_percent:.2f}%)\")\n",
    "print(f\"Validation (Dev) set size: {X_dev.shape[0]:,} ({dev_percent:.2f}%)\")\n",
    "print(f\"Testing set size: {X_test.shape[0]:,} ({test_percent:.2f}%)\")\n",
    "print(\"\\n✅ Data split successfully with stratification.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3be14b1c",
   "metadata": {},
   "source": [
    "##Feature Selection:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "371d89d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.ensemble import GradientBoostingClassifier, RandomForestClassifier\n",
    "\n",
    "\n",
    "imbalance_ratio = (y_train == 0).sum() / (y_train == 1).sum()\n",
    "class_weights = {0: 1, 1: imbalance_ratio}\n",
    "\n",
    "models = {\n",
    "    # 1. Lasso (L1 penalty)\n",
    "    'Lasso_L1': LogisticRegression(penalty='l1', solver='saga', C=0.01, class_weight=class_weights, random_state=42, max_iter=3000),\n",
    "    \n",
    "    # 2. Ridge (L2 penalty)\n",
    "    'Ridge_L2': LogisticRegression(penalty='l2', solver='saga', C=0.01, class_weight=class_weights, random_state=42, max_iter=3000),\n",
    "    \n",
    "    # 3. Linear SVM (L1 penalty) - rmoved due to long training times\n",
    "    #'SVM_L1': LinearSVC(C=0.01, penalty=\"l1\", dual=False, class_weight=class_weights, random_state=42, max_iter=2000),\n",
    "    \n",
    "    # 4. Gradient Boosting\n",
    "    'GradientBoost': GradientBoostingClassifier(n_estimators=50, max_depth=4, random_state=42),\n",
    "    \n",
    "    # 5. Random Forest\n",
    "    'RandomForest': RandomForestClassifier(n_estimators=50, max_depth=8, class_weight=class_weights, random_state=42, n_jobs=-1),\n",
    "}\n",
    "\n",
    "selection_results = {}\n",
    "print(\"Starting Consensus Feature Selection...\")\n",
    "\n",
    "for name, model in models.items():\n",
    "    print(f\"-> Fitting {name}...\")\n",
    "    \n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    if hasattr(model, 'coef_'):\n",
    "        coef = model.coef_[0] if model.coef_.ndim > 1 else model.coef_\n",
    "        selected = (np.abs(coef) > 1e-4).astype(int)\n",
    "    elif hasattr(model, 'feature_importances_'):\n",
    "        selected = (model.feature_importances_ > 1e-4).astype(int)\n",
    "    else:\n",
    "        selected = np.zeros(X_train.shape[1]).astype(int)\n",
    "    \n",
    "    selection_results[name] = selected\n",
    "\n",
    "selection_df = pd.DataFrame(selection_results, index=X_train.columns)\n",
    "selection_df.index.name = 'Feature'\n",
    "\n",
    "selection_df['Sum'] = selection_df.sum(axis=1)\n",
    "selection_df = selection_df.sort_values(by='Sum', ascending=False)\n",
    "\n",
    "TOP_N_FEATURES = 20\n",
    "print(f\"Filtering down to the absolute top {TOP_N_FEATURES} features...\")\n",
    "\n",
    "final_selected_features_df = selection_df.sort_values(\n",
    "    by='Sum', \n",
    "    ascending=False\n",
    ").head(TOP_N_FEATURES)\n",
    "\n",
    "final_selected_features = final_selected_features_df.index.tolist()\n",
    "\n",
    "print(f\"✅ Final feature count selected: {len(final_selected_features)}\")\n",
    "print(\"\\nTop 20 Features Selected:\")\n",
    "print(final_selected_features_df[['Sum']].to_markdown())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c679ccb",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_selected_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b0fb23a",
   "metadata": {},
   "outputs": [],
   "source": [
    "SELECTED_FEATURES = final_selected_features\n",
    "\n",
    "X_train_filtered = X_train[SELECTED_FEATURES]\n",
    "X_dev_filtered = X_dev[SELECTED_FEATURES]\n",
    "X_test_filtered = X_test[SELECTED_FEATURES]\n",
    "\n",
    "np.save('selected_features.npy', np.array(SELECTED_FEATURES))\n",
    "\n",
    "df_train_final = pd.concat([X_train_filtered, y_train], axis=1)\n",
    "df_dev_final = pd.concat([X_dev_filtered, y_dev], axis=1)\n",
    "df_test_final = pd.concat([X_test_filtered, y_test], axis=1)\n",
    "\n",
    "df_train_final.to_pickle('train_set_final_filtered.pkl')\n",
    "df_dev_final.to_pickle('dev_set_final_filtered.pkl')\n",
    "df_test_final.to_pickle('test_set_final_filtered.pkl')\n",
    "\n",
    "print(\"\\n Final, optimized, and filtered data sets saved for Modeling.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
